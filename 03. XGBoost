# -*- coding: utf-8 -*-
# 나눔고딕 폰트 설치
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import shap
mpl.rc("font", family="NanumGothic")
mpl.rcParams['axes.unicode_minus'] = False

from sklearn.model_selection import KFold
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score


# 데이터 불러오기(변인들이 선별된 데이터)
kbo_matches_pure_saber_vif_scale = pd.read_excel("/content/drive/MyDrive/논문(딥러닝)/최종데이터.xlsx")


# 종속변인(승패) 저장
y = kbo_matches_pure_saber_vif_scale["승1패0"]
kbo_matches_pure_saber_vif_scale = kbo_matches_pure_saber_vif_scale.drop("승1패0", axis=1)


# KFold
kfold = KFold(n_splits=5, shuffle=True, random_state=1)

# SHAP를 사용하기 위해 KFold 교차 검증 과정에서 사용된 데이터들의 인덱스 저장
ix_training, ix_test = [], []

# 각 회차에서 빈 리스트에 인덱스 저장
for fold in kfold.split(kbo_matches_pure_saber_vif_scale):
    ix_training.append(fold[0]), ix_test.append(fold[1])


============================== XGBoost ==============================
# 독립변인
feature_names = ["홈/어웨이",
                 "볼넷", "사구", "삼진", "실책", "OPS", "승계주자실점률", "ERA", "K%", "BB%",
                 "KIA", "KT", "LG", "NC", "SSG", "두산", "롯데", "삼성", "키움", "한화"]

# 변인 중요도 리스트
SHAP_values_per_fold = []
feature_imp_list = []

# 평가지표 리스트
train_accuracy_list = []
train_f1_list = []
train_precision_list = []
train_recall_list = []
test_accuracy_list = []
test_f1_list = []
test_precision_list = []
test_recall_list = []

# KFold 교차검증
for train_idx, test_idx in kfold.split(kbo_matches_pure_saber_vif_scale):

    # 학습용/평가용 데이터 분할
    X_train_fold = kbo_matches_pure_saber_vif_scale.iloc[train_idx]
    X_test_fold = kbo_matches_pure_saber_vif_scale.iloc[test_idx]
    y_train_fold = y[train_idx]
    y_test_fold = y[test_idx]

    # XGBoost 학습 및 예측 수행
    model_xgboost = XGBClassifier(random_state=1, n_estimators=200)
    model_xgboost.fit(X_train_fold, y_train_fold)
    y_train_predict = model_xgboost.predict(X_train_fold)
    y_test_predict = model_xgboost.predict(X_test_fold)

    # 학습 데이터 예측 평가지표
    train_accuracy = round(accuracy_score(y_train_fold, y_train_predict), 3)
    train_accuracy_list.append(train_accuracy)

    train_f1 = round(f1_score(y_train_fold, y_train_predict), 3)
    train_f1_list.append(train_f1)

    train_precision = round(precision_score(y_train_fold, y_train_predict), 3)
    train_precision_list.append(train_precision)

    train_recall = round(recall_score(y_train_fold, y_train_predict), 3)
    train_recall_list.append(train_recall)

    # 테스트 데이터 예측 평가지표
    test_accuracy = round(accuracy_score(y_test_fold, y_test_predict), 3)
    test_accuracy_list.append(test_accuracy)

    test_f1 = round(f1_score(y_test_fold, y_test_predict), 3)
    test_f1_list.append(test_f1)

    test_precision = round(precision_score(y_test_fold, y_test_predict), 3)
    test_precision_list.append(test_precision)

    test_recall = round(recall_score(y_test_fold, y_test_predict), 3)
    test_recall_list.append(test_recall)

    # SHAP
    explainer = shap.TreeExplainer(model_xgboost)
    shap_values = explainer.shap_values(X_test_fold)
    for shap_value in shap_values:
        SHAP_values_per_fold.append(shap_value)

    # 변인 중요도
    for i in range(X_test_fold.shape[1]):
        feature_imp = np.mean(np.abs(shap_values[:, i]))
        feature_imp_list.append(feature_imp)

============================== XGBoost ==============================


# Accuracy, Precision, Recall, F1 저장(Train data)
accuracy_score_mean = round(np.mean(train_accuracy_list, axis=0), 3)
precision_score_mean = round(np.mean(train_precision_list, axis=0), 3)
recall_score_mean = round(np.mean(train_recall_list, axis=0), 3)
f1_score_mean = round(np.mean(train_f1_list, axis=0), 3)

# 각 점수의 평균값 리스트에 저장(Train data)
train_accuracy_list.append(accuracy_score_mean)
train_precision_list.append(precision_score_mean)
train_recall_list.append(recall_score_mean)
train_f1_list.append(f1_score_mean)

# 출력(Train data)
order = ["1차", "2차", "3차", "4차", "5차", "평균"]
index = ["Accuracy", "Precision", "Recall", "F1"]
validation_df = pd.DataFrame(data=[train_accuracy_list, train_precision_list, train_recall_list, train_f1_list], columns=order, index=index)


# Accuracy, Precision, Recall, F1 저장(Test data)
accuracy_score_mean = round(np.mean(test_accuracy_list, axis=0), 3)
precision_score_mean = round(np.mean(test_precision_list, axis=0), 3)
recall_score_mean = round(np.mean(test_recall_list, axis=0), 3)
f1_score_mean = round(np.mean(test_f1_list, axis=0), 3)

# 각 점수의 평균값 리스트에 저장(Test data)
test_accuracy_list.append(accuracy_score_mean)
test_precision_list.append(precision_score_mean)
test_recall_list.append(recall_score_mean)
test_f1_list.append(f1_score_mean)

# 출력(Test data)
order = ["1차", "2차", "3차", "4차", "5차", "평균"]
index = ["Accuracy", "Precision", "Recall", "F1"]
validation_df = pd.DataFrame(data=[test_accuracy_list, test_precision_list, test_recall_list, test_f1_list], columns=order, index=index)


# SHAP 절대적 영향도 수치 출력
feature_imp_list = np.array(feature_imp_list).reshape(5, 20)
feature_imp_df = pd.DataFrame(feature_imp_list)
feature_imp_list_final = []
for i in range(feature_imp_list.shape[1]):
    feature_imp_list_final.append(np.mean(feature_imp_df.iloc[:, i]))
for i in range(len(feature_imp_list_final)):
    print("{}의 중요도: {:.4f}".format(feature_names[i], feature_imp_list_final[i]))


# SHAP 절대적 영향도 그래프 출력
shap.summary_plot(np.array(SHAP_values_per_fold), features=kbo_matches_pure_saber_vif_scale.reindex(new_index), plot_type="bar");


# SHAP 긍/부정 영향도 그래프 출력
new_index = [ix for ix_test_fold in ix_test for ix in ix_test_fold]
shap.summary_plot(np.array(SHAP_values_per_fold), features=kbo_matches_pure_saber_vif_scale.reindex(new_index));
